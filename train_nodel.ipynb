{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24733768",
   "metadata": {},
   "source": [
    "# Training a Model with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86811c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-31 01:33:02.903015: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-31 01:33:09.544362: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-05-31 01:33:09.557719: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-05-31 01:33:10.674308: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-31 01:33:13.108520: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-31 01:33:23.395053: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/mnt/d/github/Text_Summarizer/ts_venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5c8541c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-31 01:34:08.617839: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-05-31 01:34:08.791186: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-05-31 01:34:08.791235: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "# Testing GPU Support\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef91218f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK punkt for sentence tokenization\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbeee0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/github/Text_Summarizer/ts_venv/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "# loading the dataset\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\", cache_dir=\"./cache\")\n",
    "train_data = dataset['train']\n",
    "validation_data = dataset['validation']\n",
    "test_data = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ebdd0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading tokenizer_config.json: 100%|██████████| 2.32k/2.32k [00:00<00:00, 4.94MB/s]\n",
      "Downloading spiece.model: 100%|██████████| 792k/792k [00:00<00:00, 1.22MB/s]\n",
      "Downloading tokenizer.json: 100%|██████████| 1.39M/1.39M [00:00<00:00, 1.63MB/s]\n",
      "Downloading config.json: 100%|██████████| 1.21k/1.21k [00:00<00:00, 2.63MB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 242M/242M [00:43<00:00, 5.61MB/s] \n",
      "2025-05-31 01:35:17.932641: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-05-31 01:35:17.932762: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-05-31 01:35:17.932789: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-05-31 01:35:18.063126: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-05-31 01:35:18.063228: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-05-31 01:35:18.063236: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2025-05-31 01:35:18.063280: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-05-31 01:35:18.063298: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5563 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "2025-05-31 01:35:19.694168: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Loading the model and tokenizer\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=\"./cache\")\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_name, cache_dir=\"./cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eb844308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./tokenizer/tokenizer_config.json',\n",
       " './tokenizer/special_tokens_map.json',\n",
       " './tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving the tokenizer\n",
    "tokenizer.save_pretrained(\"./tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a35fb443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_function(examples):\n",
    "    # Add prefix for T5 summarization task\n",
    "    inputs = [\"summarize: \" + article for article in examples[\"article\"]]\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"tf\"\n",
    "    )\n",
    "    \n",
    "    # Tokenize summaries (targets)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            examples[\"highlights\"],\n",
    "            max_length=128,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"tf\"\n",
    "        )\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfd98c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/287113 [00:00<?, ? examples/s]/mnt/d/github/Text_Summarizer/ts_venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3856: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 287113/287113 [02:24<00:00, 1983.61 examples/s]\n",
      "Map: 100%|██████████| 13368/13368 [00:05<00:00, 2264.87 examples/s]\n",
      "Map: 100%|██████████| 11490/11490 [00:05<00:00, 2179.90 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Apply preprocessing to datasets\n",
    "train_dataset = train_data.map(preprocess_function, batched=True)\n",
    "val_dataset = validation_data.map(preprocess_function, batched=True)\n",
    "test_dataset = test_data.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42218b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to TensorFlow datasets\n",
    "def convert_to_tf_dataset(hf_dataset, batch_size=8):\n",
    "    tf_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        {\n",
    "            \"input_ids\": hf_dataset[\"input_ids\"],\n",
    "            \"attention_mask\": hf_dataset[\"attention_mask\"],\n",
    "            \"labels\": hf_dataset[\"labels\"]\n",
    "        }\n",
    "    ))\n",
    "    return tf_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "train_tf_dataset = convert_to_tf_dataset(train_dataset)\n",
    "val_tf_dataset = convert_to_tf_dataset(val_dataset)\n",
    "test_tf_dataset = convert_to_tf_dataset(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "398fd970",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-31 01:40:23.196324: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f95503ddee0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-05-31 01:40:23.197182: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 4060 Laptop GPU, Compute Capability 8.9\n",
      "2025-05-31 01:40:23.441975: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-05-31 01:40:23.737888: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1748635823.969758    1531 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35890/35890 [==============================] - 4359s 121ms/step - loss: 1.5982 - val_loss: 1.5121\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "model.compile(optimizer=optimizer)\n",
    "\n",
    "# Training the model\n",
    "history = model.fit(\n",
    "    train_tf_dataset,\n",
    "    validation_data=val_tf_dataset,\n",
    "    epochs=1,\n",
    "    batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90d87140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model\n",
    "model.save_pretrained(\"./t5_small_summarization_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f4c6280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate summaries\n",
    "def generate_summary(text, model, tokenizer, max_length=128):\n",
    "    input_text = \"summarize: \" + text\n",
    "    inputs = tokenizer(\n",
    "        input_text,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"tf\"\n",
    "    )\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_length=max_length,\n",
    "        num_beams=4,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0a3c6755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(dataset, model, tokenizer, num_samples=100):\n",
    "    predictions = []\n",
    "    references = []\n",
    "    for i, example in enumerate(dataset):\n",
    "        if i >= num_samples:\n",
    "            break\n",
    "        pred = generate_summary(example[\"article\"], model, tokenizer)\n",
    "        predictions.append(pred)\n",
    "        references.append(example[\"highlights\"])\n",
    "\n",
    "    results = rouge.compute(predictions=predictions, references=references)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d4df8931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'rouge1': 0.4, 'rouge2': 0.23655913978494622, 'rougeL': 0.35789473684210527, 'rougeLsum': 0.35789473684210527}\n"
     ]
    }
   ],
   "source": [
    "eval_results = evaluate_model(test_data.select(range(1)), model, tokenizer)\n",
    "print(\"Evaluation Results:\", eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f2a4a2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Text (truncated to 500 chars):\n",
      "\n",
      "The rapid advancement of artificial intelligence (AI) is transforming industries worldwide. From healthcare to finance, AI technologies are being used to automate tasks, improve decision-making, and enhance user experiences. In healthcare, AI is helping doctors diagnose diseases with greater accuracy through image recognition and predictive analytics. In finance, algorithms are optimizing trading strategies and detecting fraud. However, concerns about job displacement, ethical implications, and...\n",
      "\n",
      "Generated Summary:\n",
      "AI technologies are being used to automate tasks, improve decision-making, and enhance user experiences. In healthcare, AI is helping doctors diagnose diseases with greater accuracy through image recognition and predictive analytics. In finance, algorithms are optimizing trading strategies and detecting fraud. But concerns about job displacement, ethical implications, and data privacy remain significant challenges.\n"
     ]
    }
   ],
   "source": [
    "# Testing with custom text\n",
    "\n",
    "# Example custom text (replace with your own text)\n",
    "custom_text = \"\"\"\n",
    "The rapid advancement of artificial intelligence (AI) is transforming industries worldwide. From healthcare to finance, AI technologies are being used to automate tasks, improve decision-making, and enhance user experiences. In healthcare, AI is helping doctors diagnose diseases with greater accuracy through image recognition and predictive analytics. In finance, algorithms are optimizing trading strategies and detecting fraud. However, concerns about job displacement, ethical implications, and data privacy remain significant challenges. Governments and organizations are working to establish regulations to ensure AI is used responsibly.\n",
    "\"\"\"\n",
    "\n",
    "# Generate summary\n",
    "generated_summary = generate_summary(custom_text, model, tokenizer)\n",
    "print(\"Custom Text (truncated to 500 chars):\")\n",
    "print(custom_text[:500] + \"...\" if len(custom_text) > 500 else custom_text)\n",
    "print(\"\\nGenerated Summary:\")\n",
    "print(generated_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ts_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
